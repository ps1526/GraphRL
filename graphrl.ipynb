{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43298215",
   "metadata": {},
   "source": [
    "### basic q learning alg for graphs for traveling salesperson problem(Np hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c45ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/pranavsingh/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "Successfully installed scipy-1.13.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af882703",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[1;32m      3\u001b[0m Q \u001b[38;5;241m=\u001b[39m cdist(xy,xy)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy.spatial.distance import cdist\n",
    "Q = cdist(xy,xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee653e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "ENV_SIZE = 10\n",
    "N_STOPS = 100\n",
    "\n",
    "# Creating the stops using numpy random points generator \n",
    "xy = np.random.rand(N_STOPS,2)*ENV_SIZE\n",
    "\n",
    "# Computing the distances between each points\n",
    "# maytbe explore q learning for graphs in hyoerbolic space?, distance matrix can be a metric of \n",
    "# time, distance, or something else\n",
    "\n",
    "distance_matrix = cdist(xy,xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16290801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPAgent():\n",
    "    def __init__(self, states_size, action_size, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.999, gamma=0.95, lr=0.001):\n",
    "        self.state_size = states_size\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.Q = self.build_model(states_size, action_size)\n",
    "    \n",
    "    def build_model(self, states_size, action_size):\n",
    "        Q = np.zeros([states_size, action_size])\n",
    "        return Q\n",
    "    \n",
    "    def train(self, s, a, r, s_next):\n",
    "        self.Q[s,a] = self.Q[s,a] + self.lr*(r + self.gamma*np.max(self.Q[s_next, a]) - self.Q[s,a])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def act(self, s):\n",
    "        q = self.Q[s,:] \n",
    "        if np.random.rand() > self.epsilon:\n",
    "            a = np.argmax(q)\n",
    "        else:\n",
    "            a = np.random.randint(self.action_size)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3b1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env,agent,verbose = 1):\n",
    "    s = env.reset()\n",
    "    agent.reset_memory()\n",
    "    max_step = env.n_stops\n",
    "    episode_reward = 0\n",
    "    i = 0\n",
    "    while i < max_step:\n",
    "\n",
    "        # Remember the states\n",
    "        agent.remember_state(s)\n",
    "\n",
    "        # Choose an action\n",
    "        a = agent.act(s)\n",
    "        \n",
    "        # Take the action, and get the reward from environment\n",
    "        s_next,r,done = env.step(a)\n",
    "\n",
    "        # Tweak the reward\n",
    "        r = -1 * r\n",
    "        \n",
    "        if verbose: print(s_next,r,done)\n",
    "        \n",
    "        # Update our knowledge in the Q-table\n",
    "        agent.train(s,a,r,s_next)\n",
    "        \n",
    "        # Update the caches\n",
    "        episode_reward += r\n",
    "        s = s_next\n",
    "        \n",
    "        # If the episode is terminated\n",
    "        i += 1\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return env,agent,episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af0bdbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TSPEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m TSPAgent(states_size\u001b[38;5;241m=\u001b[39mN_STOPS, action_size\u001b[38;5;241m=\u001b[39mN_STOPS)\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mTSPEnv\u001b[49m(distance_matrix)\n\u001b[1;32m      3\u001b[0m run_episode(env,agent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TSPEnv' is not defined"
     ]
    }
   ],
   "source": [
    "agent = TSPAgent(states_size=N_STOPS, action_size=N_STOPS)\n",
    "env = TSPEnv(distance_matrix)\n",
    "run_episode(env,agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cb8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
